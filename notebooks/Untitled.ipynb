{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf297ef-6e04-456a-82ba-6d845012d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, files, sequence_length, window_size=25):\n",
    "        # self.folder = folder\n",
    "        self.all_folders = files\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.window_size = int(window_size)\n",
    "        self.batch_sequence_segment = np.zeros((len(self.all_folders)), dtype=np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int((len(self.all_folders)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # if idx == 0:\n",
    "        #     print(self.batch_sequence_segment[idx])\n",
    "        data = np.load(self.all_folders[idx])\n",
    "        inp, target, mask = torch.tensor(data['inp']), torch.tensor(data['target']), torch.tensor(data['mask'])\n",
    "        inp_idx, targ_idx, mask_idx = inp[self.batch_sequence_segment[idx]:self.batch_sequence_segment[idx]+self.window_size], \\\n",
    "                            target[self.batch_sequence_segment[idx]:self.batch_sequence_segment[idx]+self.window_size], \\\n",
    "                            mask[self.batch_sequence_segment[idx]:self.batch_sequence_segment[idx]+self.window_size]\n",
    "        # if self.batch_sequence_segment[idx] == inp.shape[0] - self.window_size:\n",
    "        if self.batch_sequence_segment[idx] == self.sequence_length - self.window_size:\n",
    "            self.batch_sequence_segment[idx] = 0\n",
    "        else:\n",
    "            self.batch_sequence_segment[idx] += self.window_size\n",
    "        return inp_idx, targ_idx, mask_idx, self.batch_sequence_segment[idx] == 25, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4da556-4188-47af-899d-e38a7c9449b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# data_file = '/common/users/dm1487/legged_manipulation_data/rollout_data/set_3_trajectories/rnn_200k_data.npz'\n",
    "\n",
    "# data_dict = np.load(data_file)\n",
    "# print(list(data_dict.keys()))\n",
    "\n",
    "# inp = data_dict['inp']\n",
    "# target = data_dict['target']\n",
    "# mask = data_dict['mask'][:, :750]\n",
    "# print(inp.shape, target.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed6bb5-63e0-4e22-a6fa-a404fb6f7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from pathlib import Path\n",
    "# traj_data_file = '/common/users/dm1487/legged_manipulation_data/rollout_data/set_3_trajectories/all_trajectory_files'\n",
    "# Path(traj_data_file).mkdir(parents=True, exist_ok=True)\n",
    "# for idx, (i, j, k) in tqdm(enumerate(zip(inp, target, mask))):\n",
    "#     np.savez_compressed(f'{traj_data_file}/{idx}', inp=i, target=j, mask=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3ec08-45e0-4176-98f5-d52f97830827",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_data_file = '/common/users/dm1487/legged_manipulation_data/rollout_data/set_3_trajectories/all_trajectory_files'\n",
    "\n",
    "train_idxs = np.concatenate((np.random.randint(0, 60000, 40000), np.random.randint(60000, 120000, 40000), np.random.randint(120000, 180000, 40000)), axis=-1).astype(int).tolist()\n",
    "train_idxs = np.random.randint(0, 180000, 1000).astype(int).tolist()\n",
    "\n",
    "# train_idxs = np.random.randint(0, 180000, 30000).astype(int).tolist()\n",
    "test_idxs = np.random.randint(180000, 200000, 1000).astype(int).tolist()\n",
    "\n",
    "training_files = sorted(glob(f'{traj_data_file}/*.npz'), key=lambda x: int(x.split('.npz')[0].split('/')[-1]))\n",
    "training_files = [training_files[i] for i in train_idxs]\n",
    "test_files = sorted(glob(f'{traj_data_file}/*.npz'), key=lambda x: int(x.split('.npz')[0].split('/')[-1]))\n",
    "test_files = [test_files[i] for i in test_idxs]\n",
    "\n",
    "ds = CustomDataset(files=training_files, sequence_length=250, window_size=250)\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b25d4-586f-4106-93ed-d3cc4908d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(dl))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a937e-70dc-4904-a353-f896d2366cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 2 layer LSTM with 128 hidden units with prediction of every output\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_states=None):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09378de-3e0c-4826-bebc-8333b58eb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, embed_size=512, hidden_size=2048, num_heads=8, max_sequence_length=250, num_layers=6):\n",
    "        super(MiniTransformer, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_in = nn.Linear(input_size, embed_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, \n",
    "                                                        dim_feedforward=hidden_size, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        self.linear_out = nn.Linear(embed_size, output_size)\n",
    "        self.positonal_embedding = PositionalEncoding(embed_size, max_len=max_sequence_length)\n",
    "    \n",
    "    def forward(self, x, src_mask, key_mask):\n",
    "        x = self.linear_in(x)\n",
    "        # print(x)\n",
    "        x = self.positonal_embedding(x)\n",
    "        x = self.encoder(x, mask=src_mask, src_key_padding_mask=key_mask)\n",
    "        # print(x)\n",
    "        x = self.linear_out(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894bcb9-134c-4fd2-9a5e-f309ff5b0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniTransformer(37, 14, 512, 2048, 8, 250, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97eaea-6b7d-4dc7-a167-20e582bac185",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = torch.triu(torch.ones(250, 250) * float('-inf'), diagonal=1)\n",
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f9049-6357-4e7e-97da-e5b7ecbd0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inp, targ, mask, done, idx) in enumerate(dl):\n",
    "    new_mask = torch.ones_like(mask) * float('-inf')\n",
    "    new_mask[mask.nonzero(as_tuple=True)] = 0.\n",
    "    out = model(inp, src_mask=src_mask, key_mask=new_mask)\n",
    "    print(out.shape, targ.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f659c194-0479-4a58-be19-4a4ff3e412a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed891781-c626-4c7d-9d0d-8d7e96ba9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_TRAJ = Path(f'/common/users/dm1487/legged_manipulation_data/rollout_data/latest_individual_traj_mini')\n",
    "all_files = os.listdir(DATA_PATH_TRAJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d2d8edc-9dfb-4d8b-8725-dad142da5482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 161375/161375 [10:03<00:00, 267.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "DATA_PATH_TRAJ = Path(f'/common/users/dm1487/legged_manipulation_data/rollout_data/latest_individual_traj_mini')\n",
    "all_files = os.listdir(DATA_PATH_TRAJ)\n",
    "ignore_files = []\n",
    "for i in tqdm(all_files[:]):\n",
    "    traj = np.load(DATA_PATH_TRAJ/i)\n",
    "    last_idx = traj['done'].nonzero()[0][-1]\n",
    "    if np.sum(traj['fsw'][last_idx][:7]) != 0:\n",
    "        if np.sum(traj['fsw'][last_idx][1:7] == traj['priv_obs'][last_idx][1:7]) != 6:\n",
    "            \n",
    "            ignore_files.append(DATA_PATH_TRAJ/i)\n",
    "            \n",
    "## save ignore_files as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "957a8ea0-cb62-4d81-8644-5da3b03ec991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.422308288148724"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(ignore_files)/len(all_files))*100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53a159cb-33ff-4909-afb0-e9c25e4481bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/common/users/dm1487/legged_manipulation_data/rollout_data/ignore_files_latest_individual_traj_mini.pkl', 'rb') as f:\n",
    "    ignore_files_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9c418e6-5dec-4e6d-9ee6-1315b9eb847e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29729"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ignore_files_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12597cc5-8933-40e1-a2ae-8882873c50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_ignored = [int(i.stem.split('_')[-1]) for i in ignore_files_loaded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca4fefed-a8e6-46c5-9495-d2f0e892cd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131646"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(np.arange(0, len(all_files)).tolist()) - set(idxs_ignored)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f605f72-7fc2-477b-b5fd-2da193e28fa5",
   "metadata": {},
   "source": [
    "## Transformer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4517ae16-ae10-4cd6-800c-7f8cdc26f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indep_model.model import MiniTransformer\n",
    "from indep_model.data import CustomDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from indep_model.visualization import get_visualization, get_animation\n",
    "from matplotlib import animation\n",
    "from tqdm import tqdm\n",
    "FFwriter = animation.FFMpegWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "97584d63-a4a3-42e9-81a6-1ba4daacbc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265951"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(all_train_test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e242dad-36cf-4126-88c9-aa336e4c384e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103427\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c69ccb5f-bde9-4fb7-a8b2-511d726196bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2805c99f-1dc8-47df-a88e-d644f2b8a1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2fd06fca-9b14-4c72-9d0a-5113c7dd2491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MiniTransformer(input_size=37, output_size=14, embed_size=128, hidden_size=2048, num_heads=8, max_sequence_length=250, num_layers=8)\n",
    "model = model.to('cuda:0')\n",
    "model.load_state_dict(torch.load('../indep_model/results/transformer_250_2048/2023-01-30_18-59-39/checkpoints/model_50.pt'))\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c775a-4413-4585-aa0f-6c51d237cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_data_file = '/common/users/dm1487/legged_manipulation_data/rollout_data/latest_individual_traj_1'\n",
    "all_train_test_files = sorted(glob(f'{traj_data_file}/*.npz'), key=lambda x: int(x.split('.npz')[0].split('/')[-1].split('_')[-1]))\n",
    "idxs = np.random.randint(0, len(all_train_test_files), 100).astype(int)\n",
    "for n_idx in tqdm(idxs):\n",
    "    traj = np.load(all_train_test_files[n_idx])\n",
    "    one = CustomDataset(files=[all_train_test_files[n_idx]], sequence_length=250, window_size=250)\n",
    "    inp, targ, mask, fsw, done, idx = next(iter(DataLoader(one)))\n",
    "    inp, targ, mask, fsw =  inp.to('cuda:0'), targ.to('cuda:0'), mask.to('cuda:0'), fsw.to('cuda:0')\n",
    "    src_mask = torch.triu(torch.ones(250, 250) * float('-inf'), diagonal=1).to('cuda:0')\n",
    "    patches = []\n",
    "    with torch.inference_mode():\n",
    "        for i in range(1, inp.shape[1]+1):\n",
    "            new_inp = torch.nn.functional.pad(inp[:, :i, :], (0, 0, 0, 250-i))\n",
    "            out = model(new_inp, src_mask)\n",
    "            patches.append(get_visualization(0, inp[:, i-1, :], targ[:, i-1, :], out[:, i-1, :], fsw[:, i-1, :]))\n",
    "        anim = get_animation(patches[:(mask[idx, :, :].nonzero()[:, 1][-1])])\n",
    "        anim.save(f'../indep_model/psuedo_online_sim_results/{n_idx.item()}.mp4', writer = FFwriter(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "fb3913ca-8511-4362-bfa3-103bea1a24d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2370, 13]), torch.Size([2370, 12]), torch.Size([2370, 12]))"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dof = torch.load('/common/users/dm1487/abc_3/dof_vel_tracker.zip').permute(1, 0)[:-1]\n",
    "tau = torch.load('/common/users/dm1487/abc_3/est_tau_tracker.zip').permute(1, 0)[:-1]\n",
    "obs = torch.load('/common/users/dm1487/abc_3/observation_tracker.zip').permute(1, 0)\n",
    "obs.shape, tau.shape, dof.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3f8f0a29-0e96-4eed-b99c-af419fdaf1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 37])\n",
      "torch.Size([1, 250, 13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5784d55b50>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeQElEQVR4nO3de5CV9Z3n8ffH5rIdomlRJNhCYJCQ0eDKTq+XojLlJhpIshN6nIyR0gxWoZRTk53aNUMJpbOMRhey1Gr+mOzO4uRCovGyxmnZwUihht1ZS9w0CyMxuwTwSnuBcImZpEOg/e4f5zl6OJzTfe7P6X4+r6pT/Zzf83vO+drVPl9+l+f3U0RgZmbZdVraAZiZWbqcCMzMMs6JwMws45wIzMwyzonAzCzjxqUdQC3OPvvsmDlzZtphmJmNKtu3b/95REwpLh+ViWDmzJn09/enHYaZ2agi6dVS5e4aMjPLOCcCM7OMcyIwM8s4JwIzs4xzIjAzy7hROWvIzEanvh0DrNu8mzeODnJuVycrFs6ld3532mFlnhOBmbVE344BVj22i8HjQwAMHB1k1WO7AJwMUuauITNriXWbd7+XBPIGjw+xbvPulCKyvIYkAkmLJO2WtFfSyhLnf1/S/5F0QtIXis4tlbQneS1tRDxm1n7eODpYVbm1Tt2JQFIH8A3gM8AFwBJJFxRVew24Afh+0bWTgdXApcAlwGpJZ9Ybk5m1n3O7Oqsqt9ZpRIvgEmBvRLwUEb8FHgIWF1aIiFci4gXg3aJrFwJbIuJwRBwBtgCLGhCTmbWZFQvn0jm+46SyzvEdrFg4N6WILK8RiaAbeL3g/f6krKHXSlouqV9S/8GDB2sK1MzS0zu/mzVXz6O7qxMB3V2drLl6ngeK28ComTUUEeuB9QA9PT3eaNlsFOqd3+0bfxtqRItgAJhe8P68pKzZ15qZWQM0IhH8GJgjaZakCcC1wMYKr90MfFrSmckg8aeTMjMza5G6E0FEnAC+TO4G/n+BRyLiRUl3Svo8gKR/KWk/8MfAf5X0YnLtYeCr5JLJj4E7kzIzM2sRRYy+7vaenp7wxjRmNpLr7nuOZ/e9/2/LBbMn88BNl6cYUbokbY+InuJyP1lsZmNScRIAeHbfYa6777mUImpfTgRmNiYVJ4GRyrPMicDMLOOcCMzMMs6JwMzGpAWzJ1dVnmVOBGY2Jj1w0+Wn3PSzPmuonFGzxISZWbV806+MWwRmZhnnRGBmlnFOBGZmGedEYGaWcU4EZmYZ51lDZjYqeUG5xnGLwMxGHS8o11huEZjZqNOIBeX6dgywbvNu3jg6yLldnaxYODez22g6EZhZ5vTtGGDVY7sYPD4EwMDRQVY9tgsgk8nAXUNmljnrNu9+LwnkDR4fYt3m3SlFlK6GJAJJiyTtlrRX0soS5ydKejg5/7ykmUn5TEmDknYmr79pRDxmNrbVu6DcG0cHqyof6+pOBJI6gG8AnwEuAJZIuqCo2jLgSEScD9wLfK3g3L6IuDh53VxvPGY29tW7oNy5XZ1VlY91jRgjuATYGxEvAUh6CFgM/LSgzmLgr5LjR4G/lqQGfLeZZVQ9U0VXLJx70hgBQOf4DlYsnNuI0EadRnQNdQOvF7zfn5SVrBMRJ4BfAGcl52ZJ2iHpf0j6RLkvkbRcUr+k/oMHDzYgbDPLqt753ay5eh7dXZ0I6O7qZM3V8zI5UAzpzxp6E5gREYck/R7QJ+nCiHinuGJErAfWA/T09ESL4zSzMaZ3fnfJG//tfbt48PnXGYqgQ2LJpdO5q3deChG2TiNaBAPA9IL35yVlJetIGgd8CDgUEcci4hBARGwH9gEfbUBMZmZVu71vF/dve42hyP1bcyiC+7e9xu19u1KOrLkakQh+DMyRNEvSBOBaYGNRnY3A0uT4C8AzERGSpiSDzUj6HWAO8FIDYjIzq9qDz79eVflYUXfXUESckPRlYDPQAXwrIl6UdCfQHxEbgW8C35O0FzhMLlkA/D5wp6TjwLvAzRFR+aOBZmYNlG8JVFo+VjRkjCAingCeKCr79wXHvwH+uMR1PwB+0IgYzMzq1SGVvOl3jPFJjn6y2MwsseTS6VWVjxVpzxoyM2sb+dlBWZs1pBiFfV89PT3R39+fdhhmZqOKpO0R0VNc7q4hM7OMcyIwM8s4JwIzs4xzIjAzyzgnAjOzjHMiMDPLOCcCM7OMcyIwM8s4JwIzs4xzIjAzyzgnAjOzjHMiMDPLOCcCM7OMcyIwM8u4hiQCSYsk7Za0V9LKEucnSno4Of+8pJkF51Yl5bslLWxEPGZmVrm6E0Gy+fw3gM8AFwBLJF1QVG0ZcCQizgfuBb6WXHsBuf2LLwQWAf85v5m9mZm1RiNaBJcAeyPipYj4LfAQsLiozmJgQ3L8KPApSUrKH4qIYxHxMrA3+TwzM2uRRiSCbuD1gvf7k7KSdSLiBPAL4KwKrwVA0nJJ/ZL6Dx482ICwzcwMRtFgcUSsj4ieiOiZMmVK2uGYmY0ZjUgEA8D0gvfnJWUl60gaB3wIOFThtWZm1kSNSAQ/BuZImiVpArnB341FdTYCS5PjLwDPREQk5dcms4pmAXOA/92AmMzMrELj6v2AiDgh6cvAZqAD+FZEvCjpTqA/IjYC3wS+J2kvcJhcsiCp9wjwU+AE8GcRMVRvTGZmVjnl/mE+uvT09ER/f3/aYZiZjSqStkdET3H5qBksNjOz5nAiMDPLuLrHCMzMrLyLVj/JO8feH/o8Y2IHL9yxKMWITuUWgZlZkxQnAYB3jg1x0eonU4qoNCcCM7MmKU4CI5WnxYnAzCzjnAjMzDLOicDMrEnOmFh6Vf1y5WlxIjAza5IX7lh0yk2/HWcNefqomVkTtdtNvxS3CMzMMs4tAjOzJpu1chOFq7oJeHnt59IK5xRuEZiZNVFxEgCIpLxdOBGYmTVRufWd22ndZycCM7OMcyIwM8s4DxabmTWRKN0NpAqvv+qerew58Kv33s85ZxJbbrmiAZG9r64WgaTJkrZI2pP8PLNMvaVJnT2SlhaUb5W0W9LO5HVOPfGYmbWbl9d+7pSbfqWzhoqTAMCeA7/iqnu2Niw+qL9FsBJ4OiLWSlqZvL+1sIKkycBqoIdcYtwuaWNEHEmqXBcR3nfSzMasWqeKFieBkcprVe8YwWJgQ3K8AegtUWchsCUiDic3/y1A+z9qZ2aWEfUmgqkR8WZy/BYwtUSdbuD1gvf7k7K8byfdQn8pqdJuMzMza5ARu4YkPQV8uMSp2wrfRERIqnZq7HURMSDpdOAHwJeA75aJYzmwHGDGjBlVfo2Z2egz55xJJbuB5pwzqaHfM2KLICKujIiPl3g9DrwtaRpA8vNAiY8YAKYXvD8vKSMi8j9/CXwfuGSYONZHRE9E9EyZMqXS/z4zs1Fryy1XnHLTb8asoXoHizcCS4G1yc/HS9TZDPyHghlFnwZWSRoHdEXEzyWNB/418FSd8ZiZjSmNvumXUm8iWAs8ImkZ8CpwDYCkHuDmiLgxIg5L+irw4+SaO5OyScDmJAl0kEsC99UZj5nZqHR73y4efP51hiLokFhy6XTu6p3Xku9WRDuteFGZnp6e6O/3jFMzGxtu79vF/dteO6X8+stmNDQZSNoeET3F5V5iwswsZQ8+/3pV5Y3mRGBmlrKhMj0z5cobzYnAzCxlHWUeoSpX3mhOBGZmTda3Y4AFa59h1spNLFj7DH07Bk46v+TS6SWvK1feaF591Mysifp2DLDqsV0MHh8CYODoIKse2wVA7/zcIgv5AeHiWUM9H5nMgrXP8MbRQc7t6mTFwrnvXdNInjVkZtZEC9Y+w8DRwVPKu7s6eXblJ8teV5xAADrHd7Dm6nk1JwPPGjIzS8EbJZLAcOV56zbvPikJAAweH2Ld5t0Niy3PicDMrInO7eqsqjyv1gRSCycCM7MmWrFwLp3jO04q6xzfwYqFc4e9rtYEUgsnAjOzJuqd382aq+fR3dWJyI0NVNLPX2sCqYVnDZmZNVnv/O6qB3jz9ddt3t30WUNOBGZmbaqWBFILdw2ZmWWcE4GZWcY5EZiZZZwTgZlZxjkRmJllnBOBmVnG1ZUIJE2WtEXSnuTnmWXqPSnpqKS/LyqfJel5SXslPSxpQj3xmJmNRSMtY12velsEK4GnI2IO8HTyvpR1wJdKlH8NuDcizgeOAMvqjMfMbEzJr0I6cHSQ4P1lrBuZDOpNBIuBDcnxBqC3VKWIeBr4ZWGZJAGfBB4d6Xozs6xqxSqk9T5ZPDUi3kyO3wKmVnHtWcDRiDiRvN8PlH2ETtJyYDnAjBkzqg70otVP8s6x93+ZZ0zs4IU7FlX9OWZmrdSKVUhHbBFIekrST0q8FhfWi9wON03b5SYi1kdET0T0TJkypapri5MAwDvHhrho9ZONDNHMrOFasQrpiC2CiLiy3DlJb0uaFhFvSpoGHKjiuw8BXZLGJa2C84DGjoAkipPASOVmZu1ixcK5JXcqa+QqpPWOEWwElibHS4HHK70waUH8CPhCLdebmWVBrctYV6PeMYK1wCOSlgGvAtcASOoBbo6IG5P3/wB8DPigpP3AsojYDNwKPCTpLmAH8M064zEzG3OavQppXYkgIg4BnypR3g/cWPD+E2Wufwm4pJ4YKnHGxI6S3UBnTOwoUdvMLFsy8WTxC3csOuWm71lDZmY5mdmYxjd9M7PSMtEiMDOz8pwIzMwyLjNdQ4121T1b2XPgV++9n3POJLbcckV6AZmZ1cgtghoUJwGAPQd+xVX3bE0nIDOzOjgR1KA4CYxUbmbWzpwIzMwyzonAzCzjPFhcgznnTCrZDTTnnElVfc7tfbt48PnXGYqgQ2LJpdO5q3deo8I0M6uIWwQ12HLLFafc9KudNXR73y7u3/YaQ5FbuXsogvu3vcbtfbsaGaqZ2YjcIqhRvVNFH3z+9bLlbhWYWSs5EaQk3xIoVX7R6ie9JIaZMXPlplPKXln7uYZ/j7uGUtIhlT3n3dPMrFQSGK68Hm4RpGTJpdO5f9trZc9XunvapXdv4e1f/va991NPn8Dzt11Vd3xmlh1uEaTkrt55XH/ZjLo+ozgJALz9y99y6d1b6vpcM8sWJ4IU1TsoXJwERio3MyulrkQgabKkLZL2JD/PLFPvSUlHJf19Ufl3JL0saWfyurieeEajcrukefc0M2uVelsEK4GnI2IO8HTyvpR1wJfKnFsRERcnr511xjPqePc0Myul3OygZswaqneweDFwRXK8AdhKbkP6k0TE05KuKC63nFpv+lNPn1CyG2jq6RPqDcnM2kAzbvql1NsimBoRbybHbwFTa/iMuyW9IOleSRPLVZK0XFK/pP6DBw/WFOxY8/xtV51y0/esITOr1ogtAklPAR8uceq2wjcREZJKPyVV3ipyCWQCsJ5ca+LOUhUjYn1Sh56enmq/Z8zyTd/M6jViIoiIK8udk/S2pGkR8aakacCBar68oDVxTNK3gb+o5nozs9GqnZ4BqneMYCOwFFib/Hy8mosLkoiAXuAndcZjZtb2hnsG6KoLP9zyVYnrTQRrgUckLQNeBa4BkNQD3BwRNybv/wH4GPBBSfuBZRGxGXhA0hRAwE7g5jrjsQr07Rhg3ebdvHF0kHO7OlmxcC6987vTDsssM4Z7BqhwxYH8qsRQ/3NHw6krEUTEIeBTJcr7gRsL3n+izPWfrOf7rXp9OwZY9dguBo/nlrAYODrIqsdyS187GZi1p2avSuy1hjJm3ebd7yWBvMHjQ6zbvLvmRHDdfc/x7L7D771fMHsyD9x0eV1xmtn7yq1W3CheYiJj3jg6WFX5SIqTAMCz+w5z3X3P1fR5ZllQ7bM+w61W3AhOBBlzbldnVeUjKU4CheUzV2567+Vltc3eV+4ZoHILUQ5FsGDtM/TtGGhKPO4aypgVC+eeNEYA0Dm+gxUL5zb1e/N7LHjpDLOc4qmi+T3My2nmeJ5bBBnTO7+bNVfPo7urEwHdXZ2suXpeSwaKK91jwSxrivcwLyc/ntdobhFkUO/87obd+BfMnly2e8jMKjNcS6DYQI3jecNxi8Dq8sBNl7Ng9uS0wzAb1aqZFSRo+FiBWwRWt+KpohetfrJkN5D3WDAr7TTBuxXmgoC6pnuX4kRgDffCHYtOSQat2GMhP9jWykfzzRph4rjTGDz+bsX1a53uXY4TgTVFq2cH5Qfb8lr1aL5ZI/ymiiQAtU/3LseJwMaEcoNt9297za0Ea3vndnWWHAQ+8wPj+c3xd5s+3duJwMaE4Qbb8ueKWwnnr9rEiYLLxgn2rmnNjlBmhco937P6Dy4EaPoikYomr2HRDD09PdHf3592GNZGZq96ouKZFx0SIk5KAnlOBpaWVqwKLGl7RPQUl7tFYGPCkkunnzRGMJzhEkap5GDWCo18vqdaTgQ2JuT7/QvHA96NoNR9vUMasfXgPRusldL+e3PXkI1ZxTOJ8q6/bMawrYevf/Hikv21rVqKw7KleI8QaN7fW7muIT9ZbGPWXb3zuP6yGe8t4dshcf1lM7irdx7jyqzqO07D79lg1mjt8PdWV9eQpMnAw8BM4BXgmog4UlTnYuC/AGcAQ8DdEfFwcm4W8BBwFrAd+FJElN7DzawGd/XOKzlddO+az5WdNTRr5aaSn9Xoh3jMoPF7hNSi3jGClcDTEbFW0srk/a1FdX4N/ElE7JF0LrBd0uaIOAp8Dbg3Ih6S9DfAMnJJw6zpys0OKjenu9EP8ZhBe/y91ds1tBjYkBxvAHqLK0TEzyJiT3L8BnAAmCJJwCeBR4e73qzVViycS+f4k9dFasWeDZZN7fD3Vm+LYGpEvJkcvwVMHa6ypEuACcA+ct1BRyPiRHJ6P1B2ZETScmA5wIwZpXfxMWuE/ACdZw1ZK7TD39uIs4YkPQV8uMSp24ANEdFVUPdIRJxZ5nOmAVuBpRGxTdLZwLaIOD85Px34YUR8fKSgPWvIsijtKYY2+tX8QFlEXDnMh74taVpEvJnc6A+UqXcGsAm4LSK2JcWHgC5J45JWwXlAczbkNBvliqcYNnPbQsueescINgJLk+OlwOPFFSRNAP4O+G5E5McDiFxT5EfAF4a73szaY4qhjV31JoK1wFWS9gBXJu+R1CPpb5M61wC/D9wgaWfyujg5dytwi6S95MYMvllnPGZjUrmphANHB7m9b1eLo7Gxpq7B4og4BHyqRHk/cGNyfD9wf5nrXwIuqScGsywoN8UQqHrfBW/gY8X8ZLHZKFBqimGhSjc/zy+7Ubw0t1sV2eZF58xGgfyA8L99eGfJ85UuwT3cBj4/+n8HK5qJ5BbF2ONEYDZK9M7v5iuP/GPJm35+PaWRDJcwSs1EKp6yOvOsTp7dd/ikz/OWoKOfu4bMRpEll06vqrzYSAmjcCZSfsrqwNFBglyiKEwChSrtmrL25BaB2ShSat+FarpmKtnAJz9DqdSU1XIq7ZrKK/VwXP+rh93llBInArNRptyKqpVeC+8nklLyi51Vs/plpV1TUPrhuFse3sm7BXWq7XJq13GLmSVWsn1lbftthequIbOMuat3HvvWfJavf/HiYRc7q2b1y0q7pqB0S+PdMnUr6XJq15lQpZJAvrxvxwAL1j7DrJWbWLD2Gfp2pLuoghOBWUb1zu9mzdXz6O7qREB3V+dJu2KNNGU1b9KEjqr+9V1NS6OSLqdyyaKdxy2Kx15WPbYr1WTgriGzDBtuw/R8ebmZSpBrQdz9h9V1wQz3cFyxSrqcysVW7bhFK5VbLiStdaPcIjCzsnrnd/OfrvnnJVsGZ35gfE376lba0oDKupzKJYtqxi3aQZo74LlFYGbDavR6+YWfN1zLIL+/9EjKzYSqZtyiHaS5A96I+xG0I+9HYDY2lBtQhepm14ymWUNf/+LFJ82aglwXWy2tq2rVvB+BmVm7q2dKbTMNl8zaaZMhJwIzS804wYkSnRLjRlf3ftWGG6RPgxOBmaVm75rPcf6qTSclg3HKlbfSdfc9d9LyGQtmT+aBmy5vaQxp8hiBmWXaRauf5J1jpy6lIaDw7njGxA5euGNRw743jXGNcmMEnj5qZpl13X3PlUwCcHISAHjn2BAXrX6y7u/s2zHA7/7lD9vqaWgnAjPLrHKrqZZTLmlU6v11lkovqpHW09B1JQJJkyVtkbQn+XlmiToXS3pO0ouSXpD0xYJz35H0com9jM3MxpyRVnRN62noelsEK4GnI2IO8HTyvtivgT+JiAuBRcDXJXUVnF8RERcnr511xmNm1lQzV246qQvn9r5dzF71BDNXbmL2qieG7d4Z6enhtJ6GrnfW0GLgiuR4A7AVuLWwQkT8rOD4DUkHgCnA0Tq/28ysLgtmT666ewhyW3tu3DFwSldR4fLZLx/8p5M+e+K40+j6wHiO/Pp42c9N62noelsEUyPizeT4LWDqcJUlXQJMAPYVFN+ddBndK2niMNcul9Qvqf/gwYN1hm1mBg/cdDkLZk+u6drhxgvu3/baKQnm2Il3OfLr44zvOPVf/aep8iU1mmHE6aOSngI+XOLUbcCGiOgqqHskIk4ZJ0jOTSPXYlgaEdsKyt4ilxzWA/si4s6Rgvb0UTNrhsKd05rVW9/VOZ5JE8el8lRxzUtMRMSVw3zo25KmRcSbyU39QJl6ZwCbgNvySSD57Hxr4pikbwN/MVI8ZmbNUvjE73DrINXjF4PH2bn600357FrV2zW0EViaHC8FHi+uIGkC8HfAdyPi0aJz05KfAnqBn9QZj5lZQzRrS8k0Vxktp64niyWdBTwCzABeBa6JiMOSeoCbI+JGSdcD3wZeLLj0hojYKekZcgPHAnYm1/zTSN/rriEza6Wr7tnKngO/avjntnopi3JdQ15iwsysAoVLQjRSPhnkxydK7dHQqNaJE4GZWYOUWqSulmmoeddfNoMfbB8Y9mGzRiQD70dgZtYghd05V92zta4kAPDAtteaNkupEl5ryMysRo0aO0i7X8aJwMysRs0YQE6DE4GZWRtIc1M2JwIzs5R1ju/gustm0F3mGYNmPdOQ58FiM7MazTlnUs3dQ/kd0Lq9eb2Z2ei15ZYrahowboebfyEnAjOzOmy55YpTyko9fNaqfYlr4QfKzMwywpvXm5lZSU4EZmYZ50RgZpZxTgRmZhnnRGBmlnGjctaQpIPkNsKp19nAzxvwOWlw7Olw7Olw7I3xkYiYUlw4KhNBo0jqLzWVajRw7Olw7Olw7M3lriEzs4xzIjAzy7isJ4L1aQdQB8eeDseeDsfeRJkeIzAzM7cIzMwyz4nAzCzjMpEIJC2StFvSXkkrS5y/RdJPJb0g6WlJH0kjzlIqiP1mSbsk7ZT0vyRdkEacpYwUe0G9P5IUktpmil0Fv/cbJB1Mfu87Jd2YRpylVPJ7l3RN8jf/oqTvtzrGcir4vd9b8Dv/maSjKYRZUgWxz5D0I0k7knvNZ9OIs6SIGNMvoAPYB/wOMAH4R+CCojr/CvhAcvynwMNpx11F7GcUHH8eeDLtuCuNPal3OvA/gW1AT9pxV/F7vwH467RjrTH2OcAO4Mzk/Tlpx13N30xB/X8DfCvtuKv4va8H/jQ5vgB4Je24868stAguAfZGxEsR8VvgIWBxYYWI+FFE/Dp5uw04r8UxllNJ7O8UvJ1Ebve7djBi7ImvAl8DftPK4EZQaeztqJLYbwK+ERFHACLiQItjLKfa3/sS4MGWRDaySmIP4Izk+EPAGy2Mb1hZSATdwOsF7/cnZeUsA37Y1IgqV1Hskv5M0j7gPwJ/3qLYRjJi7JL+BTA9Ija1MrAKVPo380dJE/9RSdNbE9qIKon9o8BHJT0raZukRS2LbngV/7+adN/OAp5pQVyVqCT2vwKul7QfeIJci6YtZCERVEzS9UAPsC7tWKoREd+IiNnArcDtacdTCUmnAfcAX0k7lhr9d2BmRFwEbAE2pBxPNcaR6x66gty/qu+T1JVmQDW4Fng0IobSDqQKS4DvRMR5wGeB7yX/H6SuLYJosgGg8F9r5yVlJ5F0JXAb8PmIONai2EZSUewFHgJ6mxlQFUaK/XTg48BWSa8AlwEb22TAeMTfe0QcKvg7+Vvg91oU20gq+ZvZD2yMiOMR8TLwM3KJIW3V/L1fS/t0C0FlsS8DHgGIiOeAf0ZuQbr0pT1I0YJBnHHAS+SakflBnAuL6swnN9AzJ+14a4h9TsHxHwD9acddaexF9bfSPoPFlfzepxUc/yGwLe24q4h9EbAhOT6bXJfGWaMh9qTex4BXSB6IbYdXhb/3HwI3JMe/S26MoC3+G8ZVkzRGo4g4IenLwGZyI/vfiogXJd1J7qa5kVxX0AeB/yYJ4LWI+HxqQScqjP3LSWvmOHAEWJpexO+rMPa2VGHsfy7p88AJ4DC5WUSpqzD2zcCnJf0UGAJWRMSh9KLOqeJv5lrgoUjuqO2gwti/Qq4b7t+RGzi+oV3+G7zEhJlZxmVhjMDMzIbhRGBmlnFOBGZmGedEYGaWcU4EZmYZ50RgZpZxTgRmZhn3/wGBnzYWhI3XqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# \n",
    "traj_all = torch.cat([obs, tau, dof], dim=-1)[700:-100][::6][:250]\n",
    "print(traj_all.shape)\n",
    "\n",
    "obs_1 = obs[700:-100][::6][:250].unsqueeze(0)\n",
    "print(obs_1.shape)\n",
    "plt.scatter(obs_1[0, :, 0],  obs_1[0, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3dbbb45f-7633-4f1b-92f2-58a642c202c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "c993553a-0901-4cd5-bc9b-7837a7949351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5343886"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f3b8cea3-d4be-4392-9841-52afac1f064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_all_2 = traj_all.view(-1, *traj_all.shape)\n",
    "# traj_all_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "60e2f038-7fa3-4026-8740-d41fa2d7e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_all_2 = torch.nn.functional.pad(traj_all_2, (0, 0, 0, 250-traj_all_2.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "a8a5c3fc-ec97-4c31-b940-5d42c1122bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one = CustomDataset(files=[all_train_test_files[n_idx]], sequence_length=250, window_size=250)\n",
    "# inp, targ, mask, fsw, done, idx = next(iter(DataLoader(one)))\n",
    "# inp, targ, mask, fsw =  inp.to('cuda:0'), targ.to('cuda:0'), mask.to('cuda:0'), fsw.to('cuda:0')\n",
    "traj_all_1 = traj_all_2.to('cuda:0')\n",
    "src_mask = torch.triu(torch.ones(250, 250) * float('-inf'), diagonal=1).to('cuda:0')\n",
    "patches = []\n",
    "with torch.inference_mode():\n",
    "    out = model(traj_all_1, src_mask)\n",
    "# with torch.inference_mode():\n",
    "#     for i in range(1, inp.shape[1]+1):\n",
    "#         print(i)\n",
    "#         new_inp = torch.nn.functional.pad(traj_all_1[:, :i, :], (0, 0, 0, 250-i))\n",
    "#         out = model(new_inp, src_mask)\n",
    "#         patches.append(get_visualization(0, traj_all_1[:, i-1, :], targ[:, i-1, :], out[:, i-1, :], fsw[:, i-1, :]))\n",
    "#     anim = get_animation(patches[:(mask[idx, :, :].nonzero()[:, 1][-1])])\n",
    "for i in range(1, traj_all_1.shape[1]+1):\n",
    "    patches.append(get_visualization(0, obs_1[:, i-1, :], fsw[:, i-1, :], out[:, i-1, :], fsw[:, i-1, :]))\n",
    "anim = get_animation(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f03568b7-b59f-4b52-84f4-e433b487977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim.save(f'test.mp4', writer = FFwriter(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baca6d2-bdb3-459c-aeab-4336d77c3c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
