{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf26cf0-487f-4724-b0aa-39504fa37762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c60810a-f5cc-4040-b898-92604edd6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/common/users/dm1487/legged_manipulation_data/rollout_data/random_3obstacle_data_bb_weight_move_1/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a8deeb-15ff-42e2-8360-af04459c42f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1958"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pieces = sorted(glob(str(data_path/'*.npz')))[:-1]\n",
    "print(len(all_pieces))\n",
    "idxs = np.arange(500, len(all_pieces))\n",
    "pieces = [all_pieces[i] for i in idxs]\n",
    "len(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51180c24-5485-4ca1-98e4-dd769127616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_pad_trajectories(tensor, dones):\n",
    "    \"\"\" Splits trajectories at done indices. Then concatenates them and padds with zeros up to the length og the longest trajectory.\n",
    "    Returns masks corresponding to valid parts of the trajectories\n",
    "    Example: \n",
    "        Input: [ [a1, a2, a3, a4 | a5, a6],\n",
    "                 [b1, b2 | b3, b4, b5 | b6]\n",
    "                f]\n",
    "\n",
    "        Output:[ [a1, a2, a3, a4], | [  [True, True, True, True],\n",
    "                 [a5, a6, 0, 0],   |    [True, True, False, False],\n",
    "                 [b1, b2, 0, 0],   |    [True, True, False, False],\n",
    "                 [b3, b4, b5, 0],  |    [True, True, True, False],\n",
    "                 [b6, 0, 0, 0]     |    [True, False, False, False],\n",
    "                ]                  | ]    \n",
    "            \n",
    "    Assumes that the input has the following dimension order: [time, number of envs, aditional dimensions]\n",
    "    \"\"\"\n",
    "    # dones = dones.clone()\n",
    "    dones[-1] = 1\n",
    "    # Permute the buffers to have order (num_envs, num_transitions_per_env, ...), for correct reshaping\n",
    "    flat_dones = dones.transpose(1, 0).reshape(-1, 1)\n",
    "    # flat_dones = dones.reshape(-1, 1)\n",
    "\n",
    "    # Get length of trajectory by counting the number of successive not done elements\n",
    "    done_indices = torch.cat((flat_dones.new_tensor([-1], dtype=torch.int64), flat_dones.nonzero()[:, 0]))\n",
    "    trajectory_lengths = done_indices[1:] - done_indices[:-1]\n",
    "    trajectory_lengths_list = trajectory_lengths.tolist()\n",
    "    # Extract the individual trajectories\n",
    "    trajectories = torch.split(tensor.transpose(1, 0).flatten(0, 1),trajectory_lengths_list)\n",
    "    padded_trajectories = torch.nn.utils.rnn.pad_sequence(trajectories)\n",
    "    \n",
    "    trajectory_masks = trajectory_lengths > torch.arange(0, tensor.shape[0], device=tensor.device).unsqueeze(1)\n",
    "    \n",
    "    print(trajectory_masks.shape)\n",
    "    return padded_trajectories, trajectory_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ca175f-de7d-4d0c-b273-2e043b1a534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations (25, 4055, 13)\n",
      "privileged_observations (25, 4055, 24)\n",
      "observation_histories (25, 4055, 37)\n",
      "full_seen_world (25, 4055, 24)\n",
      "dones (25, 4055, 1)\n"
     ]
    }
   ],
   "source": [
    "keys = list(np.load(pieces[0]).keys())\n",
    "one_piece = np.load(pieces[0])\n",
    "one_piece = np.load(pieces[0])\n",
    "for key in keys:\n",
    "    print(key, one_piece[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd351639-128c-4513-ab59-850ba865d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pieces(key):\n",
    "    tensor_pieces = []\n",
    "    done_pieces = []\n",
    "    start = 0\n",
    "    offset = 100\n",
    "    with tqdm(total=len(pieces)) as pbar:\n",
    "        while True:\n",
    "            tensor = None\n",
    "            if start >= len(pieces):\n",
    "                break\n",
    "            for p in pieces[start:start+offset]:\n",
    "                traj_dict = np.load(p)\n",
    "                if tensor is None:\n",
    "                    if key == \"observation_histories\":\n",
    "                        tensor = torch.tensor(traj_dict[key][:, :, -37:])\n",
    "                    else:\n",
    "                        tensor = torch.tensor(traj_dict[key])\n",
    "                    dones = torch.tensor(traj_dict['dones'], dtype=torch.bool)\n",
    "                    continue\n",
    "                \n",
    "                if key == \"observation_histories\":\n",
    "                    tensor = torch.cat([tensor, torch.tensor(traj_dict[key])[:, :, -37:]], dim=0)\n",
    "                else:\n",
    "                    tensor = torch.cat([tensor, torch.tensor(traj_dict[key])], dim=0)\n",
    "                dones = torch.cat([dones, torch.tensor(traj_dict['dones'], dtype=torch.bool)], dim=0)\n",
    "            tensor_pieces.append(tensor)\n",
    "            done_pieces.append(dones)\n",
    "            start += offset\n",
    "            pbar.update(offset)\n",
    "            \n",
    "        return tensor_pieces, done_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a64da5-0b44-4019-81aa-5ad3a19fb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tmp_path = Path(f'/common/users/dm1487/tmp/3_random_bb_weight_move')\n",
    "tmp_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b348b4ba-b56d-4216-b689-7767d08350df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [08:50,  3.77it/s]                                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [11:22,  2.93it/s]                                                                                        \n",
      "2000it [21:01,  1.59it/s]                                                                                        \n",
      "2000it [11:02,  3.02it/s]                                                                                        \n",
      "2000it [01:22, 24.28it/s]                                                                                        \n"
     ]
    }
   ],
   "source": [
    "save_dones = False\n",
    "for key in keys:\n",
    "    a, d = get_pieces(key)\n",
    "    with open(tmp_path/f'{key}.pkl', 'wb') as f:\n",
    "        pickle.dump(a, f)\n",
    "    if not save_dones:\n",
    "        with open(tmp_path/f'dones.pkl', 'wb') as f:\n",
    "            pickle.dump(d, f)\n",
    "        print('done')\n",
    "        save_dones = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe73f3c1-0f2b-4432-ad62-26ca21e56328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_traj_and_save(name, tensor_pieces, done_pieces, ctr = 0):\n",
    "    DATA_PATH_TRAJ = Path(f'/common/users/dm1487/legged_manipulation_data/rollout_data/3_obstacle/trajectories_train_bb_1/{name}')\n",
    "    DATA_PATH_TRAJ.mkdir(parents=True, exist_ok=True)\n",
    "    all_dones = torch.cat(done_pieces, dim=0)\n",
    "    all_tensor = torch.cat(tensor_pieces, dim=0)\n",
    "    all_tensor_traj = split_and_pad_trajectories(all_tensor, all_dones)\n",
    "    if name == 'dones':\n",
    "        traj = all_tensor_traj[1].permute(1, 0)\n",
    "    else:\n",
    "        traj = all_tensor_traj[0].permute(1, 0, 2)\n",
    "    start = 0\n",
    "    offset = 10000\n",
    "    with tqdm(total=traj.shape[0]) as pbar:\n",
    "        while True:\n",
    "            if start >= traj.shape[0]:\n",
    "                break\n",
    "\n",
    "            np.savez_compressed(DATA_PATH_TRAJ/f'{name}_{ctr}.npz', data=traj[(start+1):(start+offset-1)])\n",
    "            start += offset\n",
    "            pbar.update(offset)\n",
    "            ctr += 1\n",
    "    return all_tensor_traj[1], ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78fed4b-0083-431f-86f0-698cc80c3ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "done\n",
      "20\n",
      "torch.Size([10000, 142925])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [02:18, 1082.00it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 154737])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [05:37, 473.69it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 156310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [02:20, 1141.39it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 157113])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [02:30, 1061.15it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8950, 142591])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [05:23, 463.80it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "done\n",
      "20\n",
      "torch.Size([10000, 142925])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [04:56, 506.67it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 154737])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [02:01, 1321.22it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 156310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [02:36, 1024.57it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 157113])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [05:04, 525.93it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8950, 142591])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [02:20, 1067.66it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "done\n",
      "20\n",
      "torch.Size([10000, 142925])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [10:51, 230.09it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 154737])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [09:43, 274.37it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 156310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [10:02, 265.42it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 157113])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [10:12, 261.11it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8950, 142591])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [10:03, 248.58it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "done\n",
      "20\n",
      "torch.Size([10000, 142925])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [04:55, 507.60it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 154737])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [02:03, 1297.30it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 156310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [02:45, 965.58it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 157113])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [05:18, 502.42it/s]                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8950, 142591])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [01:56, 1289.15it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "done\n",
      "20\n",
      "torch.Size([10000, 142925])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [00:44, 3398.99it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 154737])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [00:43, 3681.61it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 156310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [00:42, 3785.85it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 157113])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [00:35, 4447.00it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8950, 142591])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [00:28, 5251.64it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "done\n",
      "20\n",
      "torch.Size([10000, 142925])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [00:35, 4250.40it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 154737])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [00:43, 3695.41it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 156310])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [00:50, 3155.06it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 157113])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160000it [00:49, 3229.81it/s]                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8950, 142591])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [00:40, 3743.13it/s]                                                                                    \n"
     ]
    }
   ],
   "source": [
    "with open(tmp_path/f'dones.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)\n",
    "    \n",
    "for key in [*keys[:], 'dones']:\n",
    "# for key in [*keys[:]]:\n",
    "    a = None\n",
    "    b = None\n",
    "    \n",
    "    print('loading...')\n",
    "    with open(tmp_path/f'{key}.pkl', 'rb') as f:\n",
    "        a = pickle.load(f)\n",
    "    print('done')\n",
    "    print(len(a))\n",
    "    \n",
    "    splits = 5\n",
    "    start = 0\n",
    "    offset = len(a)//splits\n",
    "    ctr = 0\n",
    "    for _ in range(splits):\n",
    "        _, ctr = convert_to_traj_and_save(key, a[start:(start+offset)], d[start:(start+offset)], ctr)\n",
    "        start += offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ddd0c60-7511-40e8-a413-489abce9c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 78/78 [5:32:21<00:00, 255.66s/it]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH_TRAJ = Path(f'/common/users/dm1487/legged_manipulation_data/rollout_data/3_obstacle/traj_list_train_bb_traj_1')\n",
    "DATA_PATH_TRAJ.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_list = {}\n",
    "for key in [*keys, 'dones']:\n",
    "    file_list[key] = sorted(list(glob(f'/common/users/dm1487/legged_manipulation_data/rollout_data/3_obstacle/trajectories_train_bb_1/{key}/*.npz')), \n",
    "                            key= lambda x: int(str(x).split('.npz')[0].split('/')[-1].split('_')[-1]))\n",
    "\n",
    "traj_ctr = 0\n",
    "with tqdm(total=len(list(file_list.values())[0])) as pbar:\n",
    "    for observations, privileged_observations, observation_histories, full_seen_world, dones in zip(*file_list.values()):\n",
    "        obs = np.load(observations)['data']\n",
    "        priv_obs = np.load(privileged_observations)['data']\n",
    "        obs_hist = np.load(observation_histories)['data']\n",
    "        fsw = np.load(full_seen_world)['data']\n",
    "        d = np.load(dones)['data']\n",
    "        for idx in range(obs.shape[0]):\n",
    "            # print(obs_hist[idx].shape, fsw[idx].shape, priv_obs[idx].shape, d[idx][:750].reshape(-1, 1).shape)\n",
    "            np.savez_compressed(DATA_PATH_TRAJ/f'traj_{traj_ctr}', obs=obs[idx], priv_obs=priv_obs[idx], obs_hist=obs_hist[idx][:, -37:], fsw=fsw[idx], done=d[idx][:750].reshape(-1, 1))\n",
    "            traj_ctr += 1\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602193a-b8b7-43d8-b1f5-a1528890304b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fc1278-9ccd-4ac1-ba04-403c4f1727c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 753525/753525 [3:04:25<00:00, 68.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "DATA_PATH_TRAJ = Path(f'/common/users/dm1487/legged_manipulation_data/rollout_data/3_obstacle/traj_list_train_bb_traj_1')\n",
    "all_files = os.listdir(DATA_PATH_TRAJ)\n",
    "ignore_files = []\n",
    "label_idx = {1: [], 2:[], 3:[]}\n",
    "for i in tqdm(all_files[:]):\n",
    "    traj = np.load(DATA_PATH_TRAJ/i)\n",
    "    \n",
    "    key_id = int(traj['fsw'][0][2]!=0) + int(traj['fsw'][0][9]!=0) + int(traj['fsw'][0][16]!=0)\n",
    "    if key_id == 0:\n",
    "        continue\n",
    "    label_idx[int(key_id)].append(DATA_PATH_TRAJ/i)\n",
    "    \n",
    "    first_idx = 0\n",
    "    if np.sum(traj['priv_obs'][first_idx][:]) > 0:\n",
    "        ignore_files.append(DATA_PATH_TRAJ/i)\n",
    "        continue\n",
    "        \n",
    "    last_idx = traj['done'].nonzero()[0][-1]\n",
    "    if np.sum(traj['fsw'][last_idx][:7]) != 0:\n",
    "        for j in range(3):\n",
    "            k = j*7 + 1\n",
    "            if np.sum(traj['fsw'][last_idx][k:k+6] == traj['priv_obs'][last_idx][k:k+6]) != 6:\n",
    "                ignore_files.append(DATA_PATH_TRAJ/i)\n",
    "                break\n",
    "## save ignore_files as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4e41823-e65b-4d7d-a670-1cceeadd06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/common/users/dm1487/legged_manipulation_data/rollout_data/3_obstacle/ignore_random_traj_bb.pkl', 'wb') as f:\n",
    "#     pickle.dump(ignore_files, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a924ed1a-fd80-4088-84b6-7118bec08132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202065"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ignore_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "855cebe1-1a1b-4456-ab1c-7cd126a12223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 736704/736704 [1:29:19<00:00, 137.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "DATA_PATH_TRAJ = Path(f'/common/users/dm1487/legged_manipulation_data/rollout_data/3_obstacle/traj_list_train_bb_traj')\n",
    "all_files = os.listdir(DATA_PATH_TRAJ)\n",
    "label_idx = {1: [], 2:[], 3:[]}\n",
    "# la = []\n",
    "ctr = 0\n",
    "for i in tqdm(all_files[:]):\n",
    "    traj = np.load(DATA_PATH_TRAJ/i)\n",
    "    key_id = int(traj['fsw'][0][2]!=0) + int(traj['fsw'][0][9]!=0) + int(traj['fsw'][0][16]!=0)\n",
    "    label_idx[int(key_id)].append(DATA_PATH_TRAJ/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d944232d-e040-4e53-80f2-10b1fb114f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/common/users/dm1487/legged_manipulation_data/rollout_data/3_obstacle/labels_random_traj_bb.pkl', 'wb') as f:\n",
    "#     pickle.dump(label_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9bdb3a8d-63a1-4c94-b186-428c1fe8b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_files = []\n",
    "with open('/common/users/dm1487/legged_manipulation_data/rollout_data/3_obstacle/ignore_random_traj_bb.pkl', 'rb') as f:\n",
    "    ignore_files = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "13f796db-1fc7-45cc-bba3-e9a52ced1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(set(label_idx[3]) - set(ignore_files))\n",
    "idxs_valid = {1: [], 2:[], 3:[]}\n",
    "for k, v in label_idx.items():\n",
    "    idxs_valid[k] = [int(i.stem.split('_')[-1]) for i in list(set(v) - set(ignore_files))]\n",
    "limit_class = min(len(idxs_valid[1][:-2000]), len(idxs_valid[2][:-2000]), len(idxs_valid[3][:-2000]))\n",
    "train_idxs_valid_all = []\n",
    "for k, v in idxs_valid.items():\n",
    "    train_idxs_valid_all.extend(np.random.choice(v[:-2000], limit_class))\n",
    "val_idxs_valid_all = []\n",
    "for k, v in idxs_valid.items():\n",
    "    val_idxs_valid_all.extend(np.random.choice(v[-2000:], 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a4dbbe47-480a-46bf-8c80-221fcd787ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370854, 6000)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idxs_valid_all), len(val_idxs_valid_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2143adf-b4b4-463a-bb9c-2fb6391822ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1cc32-9ead-4adf-ab2e-2d8a225e92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/common/users/dm1487/legged_manipulation_data/rollout_data/set_3_trajectories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a730874-5b2d-4309-8b3b-8041bdb0e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_merge = sorted(list((data_path/'obs_hist').glob('*')), key= lambda x: int(str(x).split('.npz')[0].split('_')[-1]))\n",
    "DATA_PATH_TRAJ = Path(f'/common/users/dm1487/legged_manipulation_data/rollout_data/set_3_trajectories/obs_hist_combined')\n",
    "DATA_PATH_TRAJ.mkdir(parents=True, exist_ok=True)\n",
    "# len(files_merge)\n",
    "for i in range(10, len(files_merge), 10):\n",
    "    files_length = 10\n",
    "    if i == 50:\n",
    "        files_length = 9\n",
    "    files = [np.load(files_merge[j+i]) for j in range(files_length)] # np.load(files_merge[i]), np.load(files_merge[i+1])\n",
    "    print(i, len(files))\n",
    "    final_data = np.concatenate([f['data'] for f in files], axis=0)\n",
    "    print(final_data.shape)\n",
    "    np.savez_compressed(DATA_PATH_TRAJ/f'obs_hist_combined_{i}.npz', data=final_data)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e34254-a354-4522-82a7-42aefb49d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = 200000//100000\n",
    "traj_data = {\n",
    "    'obs': None, \n",
    "    'obs_hist_combined': None, \n",
    "    'priv_obs': None, \n",
    "    'mask': None\n",
    "}\n",
    "# while length <= trajectory_length:\n",
    "for i in sorted(data_path.glob('*')):\n",
    "    name = str(i).split('/')[-1] \n",
    "    if  name == 'obs_hist' or name == 'mask1':\n",
    "        continue\n",
    "    print(i)\n",
    "    for j in sorted(glob(f'{i}/*.npz'))[:trajectories]:\n",
    "        if traj_data[name] is None:\n",
    "            traj_data[name] = np.load(j)['data']\n",
    "        else:\n",
    "            traj_data[name] = np.concatenate([traj_data[name], np.load(j)['data']], axis=0)\n",
    "        print(traj_data[name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c8588-1e55-4b00-88f6-c99d210f1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_data['obs_hist_combined'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da2396-bf3e-4248-a87b-5bf1534adc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_path = Path('/common/users/dm1487/legged_manipulation_data/rollout_data/set3_traj_200k')\n",
    "# traj_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137139ee-c9bd-4966-abeb-be3c091f883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed(traj_path, **traj_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e78113-8c66-47ed-ba40-304559d64424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traj_num = np.random.randint(0, 200000)\n",
    "# print(traj_num)\n",
    "# plt.scatter(traj_data['obs'][traj_num][:, 0], traj_data['obs'][traj_num][:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e7157-c9df-4272-8baa-28ec0200e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = 200000\n",
    "obs_hist = torch.zeros(trajectories, traj_data['obs'].shape[1],  traj_data['obs'].shape[2] + traj_data['obs_hist_combined'].shape[2])\n",
    "obs_hist[:, :, :traj_data['obs'].shape[2]] = torch.tensor(traj_data['obs'])\n",
    "obs_hist[:, :, traj_data['obs'].shape[2]:] = torch.tensor(traj_data['obs_hist_combined'])\n",
    "\n",
    "FILE_NAME = 'rnn_200k_data'\n",
    "DATA_PATH_TRAJ = Path(f'/common/users/dm1487/legged_manipulation_data/rollout_data/set_3_trajectories/{FILE_NAME}')\n",
    "np.savez_compressed(DATA_PATH_TRAJ, inp=obs_hist.numpy(), target=traj_data['priv_obs'], mask=traj_data['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a0f73-a362-4d65-8078-f47a0b1d0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA_PATH_TRAJ)\n",
    "traj = np.load(DATA_PATH_TRAJ/'traj_3.npz')['obs']\n",
    "plt.scatter(traj[:, 0], traj[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5cc21-30b8-476b-903f-ada6edd2a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ab41b-0534-406e-bd7c-d32746615e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e30869-a50a-46d3-9b0f-ad4e5a2292bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1196c6c-0829-4455-b359-2b260aed17ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63668b07-5ba1-457b-9db2-671f533ad8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f67671-7a9a-4826-bb77-241301b1a288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
